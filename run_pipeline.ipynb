{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pharmaceutical-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore, Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep, PythonScriptStep\n",
    "from datetime import datetime\n",
    "import toml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-scholar",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "russian-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "with open('secrets.toml', 'r') as f:\n",
    "    secrets = toml.load(f)\n",
    "config = {**config, **secrets}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-private",
   "metadata": {},
   "source": [
    "## Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demanding-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_workspace(subscription_id: str, resource_group: str, aml_workspace_name: str, tenant_id: str = None) \\\n",
    "        -> Workspace:\n",
    "    interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "    workspace = Workspace(subscription_id, resource_group, aml_workspace_name, auth=interactive_auth)\n",
    "    return workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funded-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "ws = connect_to_workspace(config['subscription_id'], config['resource_group'], config['aml_workspace'], config['tenant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-software",
   "metadata": {},
   "source": [
    "## Define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "similar-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'parallelization_tutorial'\n",
    "compute_target_cpu = ws.compute_targets['kmi-cmpclstr-cpu']\n",
    "jobs_per_node_cpu = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-dressing",
   "metadata": {},
   "source": [
    "## Create an execution environment (only necessary once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpine-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_environment(workspace, name, base_environment, conda_dependencies, pip_dependencies, wait_for_completion=True):\n",
    "#     env = Environment.get(workspace=workspace, name=base_environment).clone(name)\n",
    "#     conda_dep = env.python.conda_dependencies\n",
    "#     for dep in conda_dependencies:\n",
    "#         conda_dep.add_conda_package(dep)\n",
    "#     for dep in pip_dependencies:\n",
    "#         conda_dep.add_pip_package(dep)\n",
    "#     env.python.conda_dependencies=conda_dep\n",
    "#     env.register(workspace=workspace)\n",
    "#     if wait_for_completion:\n",
    "#         env.build(workspace).wait_for_completion()\n",
    "        \n",
    "# name = 'tutorial-environment'\n",
    "# base_environment = 'AzureML-Minimal'\n",
    "# conda_dependencies = ['numpy', 'pandas']\n",
    "# pip_dependencies = ['opencensus-ext-azure', 'parse', 'tqdm']\n",
    "# create_environment(ws, name, base_environment, conda_dependencies, pip_dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-smith",
   "metadata": {},
   "source": [
    "## Get a handle of the execution environment (if it already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "major-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.get(workspace=ws, name='tutorial-environment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-joseph",
   "metadata": {},
   "source": [
    "## Configure environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorporate-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.environment_variables = {\n",
    "    'APPLICATIONINSIGHTS_CONNECTION_STRING': config['app_insights_connection_string']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-mortality",
   "metadata": {},
   "source": [
    "## Get a handle to the datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noted-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = 'tutorial_parallelization'\n",
    "datastore = Datastore(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-sarah",
   "metadata": {},
   "source": [
    "## Define the partitioning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "premier-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the run configuration\n",
    "run_configuration = RunConfiguration()\n",
    "run_configuration.environment = environment\n",
    "\n",
    "# define the input dataset\n",
    "ds_raw = Dataset.get_by_name(workspace=ws, name='tutorial-parallelization-raw')\n",
    "\n",
    "# define the output dataset\n",
    "ds_partitioned = OutputFileDatasetConfig(destination=(datastore, 'partitioned/{run-id}')) \\\n",
    "    .register_on_complete(name='tutorial-parallelization-partitioned')\n",
    "\n",
    "# define the step\n",
    "partition_step = PythonScriptStep(\n",
    "    name='partition-step',\n",
    "    source_directory='.',\n",
    "    script_name='partition_step.py',\n",
    "    compute_target=compute_target_cpu,\n",
    "    arguments=['--output-dir', ds_partitioned.as_mount()],\n",
    "    inputs=[ds_raw.as_named_input('ds_raw').as_mount()],\n",
    "    runconfig=run_configuration,\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-minimum",
   "metadata": {},
   "source": [
    "## Define the parallelized processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minus-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output dataset\n",
    "ds_processed = OutputFileDatasetConfig(destination=(datastore, 'processed/{run-id}'))\\\n",
    "    .register_on_complete(name='tutorial-parallelization-processed')\n",
    "\n",
    "# define the run configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='.',\n",
    "    entry_script='processing_step.py',\n",
    "    mini_batch_size=1,\n",
    "    error_threshold=0,\n",
    "    output_action='summary_only',\n",
    "    environment=environment,\n",
    "    compute_target=compute_target_cpu,\n",
    "    process_count_per_node=jobs_per_node_cpu,\n",
    "    node_count=compute_target_cpu.get_status().scale_settings.maximum_node_count,\n",
    "    run_invocation_timeout=300,\n",
    "    run_max_try=1\n",
    ")\n",
    "\n",
    "# define the step\n",
    "processing_step = ParallelRunStep(\n",
    "    name='processing-step',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--output-dir', ds_processed.as_mount()],\n",
    "    inputs=[ds_partitioned.as_input('ds_partitioned').as_mount()],\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-memory",
   "metadata": {},
   "source": [
    "## Define the aggregation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bacterial-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the run configuration\n",
    "run_configuration = RunConfiguration()\n",
    "run_configuration.environment = environment\n",
    "\n",
    "# define the output dataset\n",
    "ds_aggregated = OutputFileDatasetConfig(destination=(datastore, 'aggregated/{run-id}'))\\\n",
    "    .register_on_complete(name='tutorial-parallelization-aggregated')\n",
    "\n",
    "# define the step\n",
    "aggregation_step = PythonScriptStep(\n",
    "    name='aggregation-step',\n",
    "    source_directory='.',\n",
    "    script_name='aggregation_step.py',\n",
    "    compute_target=compute_target_cpu,\n",
    "    arguments=['--output-dir', ds_aggregated.as_mount()],\n",
    "    inputs=[ds_processed.as_input('ds_processed').as_mount()],\n",
    "    runconfig=run_configuration,\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-jenny",
   "metadata": {},
   "source": [
    "## Define the final pipeline and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broken-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[partition_step, processing_step, aggregation_step])\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-breathing",
   "metadata": {},
   "source": [
    "## Submit the experiment and wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment submitted at 2021-01-26 14:55:37\n",
      "Created step partition-step [db86f90f][9b0c8590-ae13-4e4b-a713-04bec01e3f5d], (This step will run and generate new outputs)Created step processing-step [25c4275d][e9993f05-199f-435e-88ca-5356ffff11e4], (This step will run and generate new outputs)\n",
      "\n",
      "Created step aggregation-step [a019c7e2][8844e403-dcb0-46a2-978c-258d21998ab3], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 8e766113-3415-4d6d-9d5d-24c46ba67ade\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/parallelization_tutorial/runs/8e766113-3415-4d6d-9d5d-24c46ba67ade?wsid=/subscriptions/c22bcbea-3647-4c9e-96b6-6a734c897619/resourcegroups/RG-EUR-WW-DEV-TAGDATAAIRMC/workspaces/ml-rmc\n",
      "PipelineRunId: 8e766113-3415-4d6d-9d5d-24c46ba67ade\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/parallelization_tutorial/runs/8e766113-3415-4d6d-9d5d-24c46ba67ade?wsid=/subscriptions/c22bcbea-3647-4c9e-96b6-6a734c897619/resourcegroups/RG-EUR-WW-DEV-TAGDATAAIRMC/workspaces/ml-rmc\n"
     ]
    }
   ],
   "source": [
    "print(f'Experiment submitted at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=False)\n",
    "print(f'Experiment terminated at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-catering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
