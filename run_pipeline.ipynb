{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore, Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep, PythonScriptStep\n",
    "from datetime import datetime\n",
    "import toml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-galaxy",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "with open('secrets.toml', 'r') as f:\n",
    "    secrets = toml.load(f)\n",
    "config = {**config, **secrets}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-kelly",
   "metadata": {},
   "source": [
    "## Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_workspace(subscription_id: str, resource_group: str, aml_workspace_name: str, tenant_id: str = None) \\\n",
    "        -> Workspace:\n",
    "    interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "    workspace = Workspace(subscription_id, resource_group, aml_workspace_name, auth=interactive_auth)\n",
    "    return workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = connect_to_workspace(config['subscription_id'], config['resource_group'], config['aml_workspace'], config['tenant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-background",
   "metadata": {},
   "source": [
    "## Define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'parallelization_tutorial'\n",
    "compute_target_cpu = ws.compute_targets['kmi-cmpclstr-cpu']\n",
    "jobs_per_node_cpu = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-indication",
   "metadata": {},
   "source": [
    "## Create an execution environment (only necessary once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_environment(workspace, name, base_environment, conda_dependencies, pip_dependencies, wait_for_completion=True):\n",
    "#     env = Environment.get(workspace=workspace, name=base_environment).clone(name)\n",
    "#     conda_dep = env.python.conda_dependencies\n",
    "#     for dep in conda_dependencies:\n",
    "#         conda_dep.add_conda_package(dep)\n",
    "#     for dep in pip_dependencies:\n",
    "#         conda_dep.add_pip_package(dep)\n",
    "#     env.python.conda_dependencies=conda_dep\n",
    "#     env.register(workspace=workspace)\n",
    "#     if wait_for_completion:\n",
    "#         env.build(workspace).wait_for_completion()\n",
    "        \n",
    "# name = 'tutorial-environment'\n",
    "# base_environment = 'AzureML-Minimal'\n",
    "# conda_dependencies = ['numpy', 'pandas']\n",
    "# pip_dependencies = ['opencensus-ext-azure', 'parse', 'tqdm']\n",
    "# create_environment(ws, name, base_environment, conda_dependencies, pip_dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-alloy",
   "metadata": {},
   "source": [
    "## Get a handle of the execution environment (if it already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.get(workspace=ws, name='tutorial-environment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-concentration",
   "metadata": {},
   "source": [
    "## Configure environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.environment_variables = {\n",
    "    'APPLICATIONINSIGHTS_CONNECTION_STRING': config['app_insights_connection_string']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-freeze",
   "metadata": {},
   "source": [
    "## Get a handle to the datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = 'tutorial_parallelization'\n",
    "datastore = Datastore(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-dancing",
   "metadata": {},
   "source": [
    "## Define the partitioning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the run configuration\n",
    "run_configuration = RunConfiguration()\n",
    "run_configuration.environment = environment\n",
    "\n",
    "# define the input dataset\n",
    "ds_raw = Dataset.get_by_name(workspace=ws, name='tutorial-parallelization-raw')\n",
    "\n",
    "# define the output dataset\n",
    "ds_partitioned = OutputFileDatasetConfig(destination=(datastore, 'partitioned/{run-id}')) \\\n",
    "    .register_on_complete(name='tutorial-parallelization-partitioned')\n",
    "\n",
    "# define the step\n",
    "partition_step = PythonScriptStep(\n",
    "    name='partition-step',\n",
    "    source_directory='.',\n",
    "    script_name='partition_step.py',\n",
    "    compute_target=compute_target_cpu,\n",
    "    arguments=['--output-dir', ds_partitioned.as_mount()],\n",
    "    inputs=[ds_raw.as_named_input('ds_raw').as_mount()],\n",
    "    runconfig=run_configuration,\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-dominican",
   "metadata": {},
   "source": [
    "## Define the parallelized processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output dataset\n",
    "ds_processed = OutputFileDatasetConfig(destination=(datastore, 'processed/{run-id}'))\\\n",
    "    .register_on_complete(name='tutorial-parallelization-processed')\n",
    "\n",
    "# define the run configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='.',\n",
    "    entry_script='processing_step.py',\n",
    "    mini_batch_size=1,\n",
    "    error_threshold=0,\n",
    "    output_action='summary_only',\n",
    "    environment=environment,\n",
    "    compute_target=compute_target_cpu,\n",
    "    process_count_per_node=jobs_per_node_cpu,\n",
    "    node_count=compute_target_cpu.get_status().scale_settings.maximum_node_count,\n",
    "    run_invocation_timeout=300,\n",
    "    run_max_try=1\n",
    ")\n",
    "\n",
    "# define the step\n",
    "processing_step = ParallelRunStep(\n",
    "    name='processing-step',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--output-dir', ds_processed.as_mount()],\n",
    "    inputs=[ds_partitioned.as_input('ds_partitioned').as_mount()],\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-switzerland",
   "metadata": {},
   "source": [
    "## Define the aggregation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the run configuration\n",
    "run_configuration = RunConfiguration()\n",
    "run_configuration.environment = environment\n",
    "\n",
    "# define the output dataset\n",
    "ds_aggregated = OutputFileDatasetConfig(destination=(datastore, 'aggregated/{run-id}'))\\\n",
    "    .register_on_complete(name='tutorial-parallelization-aggregated')\n",
    "\n",
    "# define the step\n",
    "aggregation_step = PythonScriptStep(\n",
    "    name='aggregation-step',\n",
    "    source_directory='.',\n",
    "    script_name='aggregation_step.py',\n",
    "    compute_target=compute_target_cpu,\n",
    "    arguments=['--output-dir', ds_aggregated.as_mount()],\n",
    "    inputs=[ds_processed.as_input('ds_processed').as_mount()],\n",
    "    runconfig=run_configuration,\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-reserve",
   "metadata": {},
   "source": [
    "## Define the final pipeline and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[partition_step, processing_step, aggregation_step])\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-kansas",
   "metadata": {},
   "source": [
    "## Submit the experiment and wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Experiment submitted at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=False)\n",
    "print(f'Experiment terminated at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-trinity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
